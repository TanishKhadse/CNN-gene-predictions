{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch_geometric\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Dataset.get_summary of Cora()>\n",
      "Data Info:  Cora\n",
      "Number of graphs:  1\n",
      "Number of features:  1433\n",
      "Number of classes (node types):  7\n",
      "Number of edge features:  0\n",
      "Number of node features:  1433\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, \n",
    "                      num_test=0.1, \n",
    "                      is_undirected=True, \n",
    "                      add_negative_train_samples=False)\n",
    "])\n",
    "\n",
    "# data\n",
    "dataset = Planetoid(root=\"testdata\", name=\"Cora\", transform=transform)\n",
    "\n",
    "\n",
    "print(\"Data Info: \", dataset.name)\n",
    "print(\"Number of graphs: \", len(dataset))\n",
    "print(\"Number of features: \", dataset.num_features)\n",
    "print(\"Number of classes (node types): \", dataset.num_classes)\n",
    "print(\"Number of edge features: \", dataset.num_edge_features)\n",
    "print(\"Number of node features: \", dataset.num_node_features )\n",
    "\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNN Model Implementation\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "    \n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(dataset.num_features, 128, 64)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index,\n",
    "        num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1),\n",
    "        method='sparse'\n",
    "    )\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    edge_label = torch.cat(\n",
    "        [train_data.edge_label, train_data.edge_label.new_zeros(neg_edge_index.size(1))],\n",
    "        dim=0\n",
    "    )\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4133, Val: 0.9399, Test: 0.9062\n",
      "Epoch: 002, Loss: 0.4125, Val: 0.9402, Test: 0.9068\n",
      "Epoch: 003, Loss: 0.4235, Val: 0.9405, Test: 0.9076\n",
      "Epoch: 004, Loss: 0.4193, Val: 0.9401, Test: 0.9072\n",
      "Epoch: 005, Loss: 0.4200, Val: 0.9404, Test: 0.9070\n",
      "Epoch: 006, Loss: 0.4185, Val: 0.9400, Test: 0.9065\n",
      "Epoch: 007, Loss: 0.4165, Val: 0.9398, Test: 0.9071\n",
      "Epoch: 008, Loss: 0.4268, Val: 0.9396, Test: 0.9074\n",
      "Epoch: 009, Loss: 0.4137, Val: 0.9395, Test: 0.9080\n",
      "Epoch: 010, Loss: 0.4237, Val: 0.9399, Test: 0.9082\n",
      "Epoch: 011, Loss: 0.4174, Val: 0.9407, Test: 0.9080\n",
      "Epoch: 012, Loss: 0.4266, Val: 0.9400, Test: 0.9080\n",
      "Epoch: 013, Loss: 0.4152, Val: 0.9395, Test: 0.9085\n",
      "Epoch: 014, Loss: 0.4144, Val: 0.9400, Test: 0.9098\n",
      "Epoch: 015, Loss: 0.4191, Val: 0.9403, Test: 0.9108\n",
      "Epoch: 016, Loss: 0.4250, Val: 0.9407, Test: 0.9109\n",
      "Epoch: 017, Loss: 0.4231, Val: 0.9409, Test: 0.9109\n",
      "Epoch: 018, Loss: 0.4186, Val: 0.9408, Test: 0.9114\n",
      "Epoch: 019, Loss: 0.4173, Val: 0.9402, Test: 0.9112\n",
      "Epoch: 020, Loss: 0.4139, Val: 0.9393, Test: 0.9110\n",
      "Epoch: 021, Loss: 0.4159, Val: 0.9386, Test: 0.9113\n",
      "Epoch: 022, Loss: 0.4178, Val: 0.9395, Test: 0.9115\n",
      "Epoch: 023, Loss: 0.4157, Val: 0.9408, Test: 0.9118\n",
      "Epoch: 024, Loss: 0.4143, Val: 0.9415, Test: 0.9117\n",
      "Epoch: 025, Loss: 0.4158, Val: 0.9418, Test: 0.9116\n",
      "Epoch: 026, Loss: 0.4227, Val: 0.9417, Test: 0.9113\n",
      "Epoch: 027, Loss: 0.4170, Val: 0.9408, Test: 0.9115\n",
      "Epoch: 028, Loss: 0.4148, Val: 0.9414, Test: 0.9124\n",
      "Epoch: 029, Loss: 0.4165, Val: 0.9424, Test: 0.9126\n",
      "Epoch: 030, Loss: 0.4221, Val: 0.9427, Test: 0.9125\n",
      "Epoch: 031, Loss: 0.4068, Val: 0.9433, Test: 0.9126\n",
      "Epoch: 032, Loss: 0.4072, Val: 0.9432, Test: 0.9128\n",
      "Epoch: 033, Loss: 0.4167, Val: 0.9422, Test: 0.9123\n",
      "Epoch: 034, Loss: 0.4131, Val: 0.9421, Test: 0.9120\n",
      "Epoch: 035, Loss: 0.4167, Val: 0.9434, Test: 0.9130\n",
      "Epoch: 036, Loss: 0.4151, Val: 0.9438, Test: 0.9137\n",
      "Epoch: 037, Loss: 0.4210, Val: 0.9439, Test: 0.9145\n",
      "Epoch: 038, Loss: 0.4122, Val: 0.9433, Test: 0.9142\n",
      "Epoch: 039, Loss: 0.4090, Val: 0.9429, Test: 0.9136\n",
      "Epoch: 040, Loss: 0.4146, Val: 0.9437, Test: 0.9139\n",
      "Epoch: 041, Loss: 0.4174, Val: 0.9443, Test: 0.9142\n",
      "Epoch: 042, Loss: 0.4086, Val: 0.9454, Test: 0.9150\n",
      "Epoch: 043, Loss: 0.4140, Val: 0.9448, Test: 0.9150\n",
      "Epoch: 044, Loss: 0.4118, Val: 0.9444, Test: 0.9151\n",
      "Epoch: 045, Loss: 0.4110, Val: 0.9456, Test: 0.9157\n",
      "Epoch: 046, Loss: 0.4072, Val: 0.9464, Test: 0.9153\n",
      "Epoch: 047, Loss: 0.4149, Val: 0.9461, Test: 0.9149\n",
      "Epoch: 048, Loss: 0.4101, Val: 0.9456, Test: 0.9149\n",
      "Epoch: 049, Loss: 0.4069, Val: 0.9461, Test: 0.9158\n",
      "Epoch: 050, Loss: 0.4157, Val: 0.9458, Test: 0.9159\n",
      "Epoch: 051, Loss: 0.4167, Val: 0.9448, Test: 0.9152\n",
      "Epoch: 052, Loss: 0.4159, Val: 0.9450, Test: 0.9144\n",
      "Epoch: 053, Loss: 0.4129, Val: 0.9441, Test: 0.9140\n",
      "Epoch: 054, Loss: 0.4141, Val: 0.9450, Test: 0.9150\n",
      "Epoch: 055, Loss: 0.4073, Val: 0.9456, Test: 0.9159\n",
      "Epoch: 056, Loss: 0.4143, Val: 0.9447, Test: 0.9159\n",
      "Epoch: 057, Loss: 0.4072, Val: 0.9436, Test: 0.9148\n",
      "Epoch: 058, Loss: 0.4098, Val: 0.9441, Test: 0.9150\n",
      "Epoch: 059, Loss: 0.4109, Val: 0.9453, Test: 0.9167\n",
      "Epoch: 060, Loss: 0.4155, Val: 0.9458, Test: 0.9176\n",
      "Epoch: 061, Loss: 0.4140, Val: 0.9448, Test: 0.9168\n",
      "Epoch: 062, Loss: 0.4129, Val: 0.9439, Test: 0.9159\n",
      "Epoch: 063, Loss: 0.4061, Val: 0.9448, Test: 0.9156\n",
      "Epoch: 064, Loss: 0.4104, Val: 0.9452, Test: 0.9155\n",
      "Epoch: 065, Loss: 0.4098, Val: 0.9447, Test: 0.9158\n",
      "Epoch: 066, Loss: 0.4113, Val: 0.9427, Test: 0.9153\n",
      "Epoch: 067, Loss: 0.4132, Val: 0.9427, Test: 0.9146\n",
      "Epoch: 068, Loss: 0.4010, Val: 0.9434, Test: 0.9134\n",
      "Epoch: 069, Loss: 0.4093, Val: 0.9433, Test: 0.9132\n",
      "Epoch: 070, Loss: 0.4138, Val: 0.9415, Test: 0.9134\n",
      "Epoch: 071, Loss: 0.4062, Val: 0.9391, Test: 0.9132\n",
      "Epoch: 072, Loss: 0.4144, Val: 0.9408, Test: 0.9131\n",
      "Epoch: 073, Loss: 0.4124, Val: 0.9415, Test: 0.9124\n",
      "Epoch: 074, Loss: 0.4136, Val: 0.9408, Test: 0.9120\n",
      "Epoch: 075, Loss: 0.4073, Val: 0.9393, Test: 0.9114\n",
      "Epoch: 076, Loss: 0.4053, Val: 0.9386, Test: 0.9120\n",
      "Epoch: 077, Loss: 0.4076, Val: 0.9398, Test: 0.9130\n",
      "Epoch: 078, Loss: 0.4032, Val: 0.9403, Test: 0.9129\n",
      "Epoch: 079, Loss: 0.4158, Val: 0.9408, Test: 0.9124\n",
      "Epoch: 080, Loss: 0.4101, Val: 0.9405, Test: 0.9124\n",
      "Epoch: 081, Loss: 0.4115, Val: 0.9403, Test: 0.9126\n",
      "Epoch: 082, Loss: 0.4049, Val: 0.9404, Test: 0.9132\n",
      "Epoch: 083, Loss: 0.4102, Val: 0.9411, Test: 0.9135\n",
      "Epoch: 084, Loss: 0.4054, Val: 0.9421, Test: 0.9131\n",
      "Epoch: 085, Loss: 0.4079, Val: 0.9428, Test: 0.9127\n",
      "Epoch: 086, Loss: 0.4135, Val: 0.9420, Test: 0.9122\n",
      "Epoch: 087, Loss: 0.4123, Val: 0.9426, Test: 0.9127\n",
      "Epoch: 088, Loss: 0.4083, Val: 0.9430, Test: 0.9131\n",
      "Epoch: 089, Loss: 0.4034, Val: 0.9436, Test: 0.9135\n",
      "Epoch: 090, Loss: 0.4181, Val: 0.9435, Test: 0.9140\n",
      "Epoch: 091, Loss: 0.4165, Val: 0.9414, Test: 0.9131\n",
      "Epoch: 092, Loss: 0.4099, Val: 0.9406, Test: 0.9121\n",
      "Epoch: 093, Loss: 0.4148, Val: 0.9421, Test: 0.9119\n",
      "Epoch: 094, Loss: 0.4059, Val: 0.9442, Test: 0.9128\n",
      "Epoch: 095, Loss: 0.4151, Val: 0.9440, Test: 0.9132\n",
      "Epoch: 096, Loss: 0.4117, Val: 0.9418, Test: 0.9129\n",
      "Epoch: 097, Loss: 0.4118, Val: 0.9398, Test: 0.9113\n",
      "Epoch: 098, Loss: 0.4097, Val: 0.9406, Test: 0.9105\n",
      "Epoch: 099, Loss: 0.4119, Val: 0.9422, Test: 0.9109\n",
      "Epoch: 100, Loss: 0.4141, Val: 0.9424, Test: 0.9114\n",
      "Final Test: 0.9153\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1,101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, ' f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "        [   0,    2,    7,  ..., 2703, 2706, 2707]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
